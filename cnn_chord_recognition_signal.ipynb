{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOwusucilsKuBCwnOYYv7ID",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShakhovaP/musical-chord-recognition/blob/main/cnn_chord_recognition_signal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYa4NcW_6KbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "bDIgSyLqg3X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0oaEv6ZlSZT",
        "outputId": "d997957f-cb6a-47a0-8319-521080098078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBSgq6a1gbyE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "DATASET_PATH = '/content/gdrive/MyDrive/dataset-2.json'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\n",
        "            \"A\",\n",
        "            \"A#m|Bbm\",\n",
        "            \"A#|Bb\",\n",
        "            \"Am\",\n",
        "            \"B\",\n",
        "            \"Bm\",\n",
        "            \"C\",\n",
        "            \"C#m|Dbm\",\n",
        "            \"C#|Db\",\n",
        "            \"Cm\",\n",
        "            \"D\",\n",
        "            \"D#m|Ebm\",\n",
        "            \"D#|Eb\",\n",
        "            \"Dm\",\n",
        "            \"E\",\n",
        "            \"Em\",\n",
        "            \"F\",\n",
        "            \"F#m|Gbm\",\n",
        "            \"F#|Gb\",\n",
        "            \"Fm\",\n",
        "            \"G\",\n",
        "            \"G#m|Abm\",\n",
        "            \"G#|Ab\",\n",
        "            \"Gm\",\n",
        "            \"N\",\n",
        "                    ]\n",
        "\n",
        "# def vectorize(number):\n",
        "#     vector = [0 for _ in range(25)]\n",
        "#     vector[number] = 1\n",
        "#     return vector\n",
        "\n",
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        \n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    # X = np.array(data[\"MFCCs\"])\n",
        "    X = np.array(data[\"signal\"])\n",
        "    y = np.array(data[\"chords\"])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for train and validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    \"\"\"Loads data and splits it into train, validation and test sets.\n",
        "\n",
        "    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n",
        "    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n",
        "\n",
        "    :return X_train (ndarray): Input training set\n",
        "    :return X_validation (ndarray): Input validation set\n",
        "    :return X_test (ndarray): Input test set\n",
        "    :return y_train (ndarray): Target training set\n",
        "    :return y_validation (ndarray): Target validation set\n",
        "    :return y_test (ndarray): Target test set\n",
        "    \"\"\"\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(DATASET_PATH)\n",
        "\n",
        "    # create train, validation and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # add an axis to input sets\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_validation = X_validation[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    \"\"\"Generates CNN model\n",
        "\n",
        "    :param input_shape (tuple): Shape of input set\n",
        "    :return model: CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    model.add(keras.layers.Conv1D(32, 3, activation='relu', input_shape=input_shape, padding='same'))\n",
        "    model.add(keras.layers.MaxPooling2D(4, padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(keras.layers.Conv1D(32, 3, activation='relu', input_shape=input_shape, padding='same'))\n",
        "    model.add(keras.layers.MaxPooling2D(4, padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(keras.layers.Conv1D(32, 3, activation='relu', input_shape=input_shape, padding='same'))\n",
        "    model.add(keras.layers.MaxPooling2D(4, padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(keras.layers.Flatten())\n",
        "\n",
        "    model.add(keras.layers.Dense(25, activation='softmax'))\n",
        "\n",
        "    \n",
        "\n",
        "    model.add(keras.layers.Dense(25, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, X, y):\n",
        "    \"\"\"Predict a single sample using the trained model\n",
        "\n",
        "    :param model: Trained classifier\n",
        "    :param X: Input data\n",
        "    :param y (int): Target\n",
        "    \"\"\"\n",
        "\n",
        "    # add a dimension to input data for sample - model.predict() expects a 4d array in this case\n",
        "    X = X[np.newaxis, ...] \n",
        "\n",
        "    # perform prediction\n",
        "    prediction = model.predict(X)\n",
        "\n",
        "    # get index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1)\n",
        "    print(\"Probabilities:\\n\", prediction)\n",
        "    index_t = int(np.where(y==1)[0])\n",
        "    index_p = int(predicted_index[0])\n",
        "    print(f\"\\nTarget: {labels[index_t]}, Predicted label: {labels[index_p]}\")\n",
        "    \n",
        "   \n",
        "    pred = np.array(prediction)\n",
        "    sorted_p = pred.sort()\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # get train, validation, test splits\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.3, 0.2)\n",
        "    \n",
        "    mean = np.mean(X_train, axis=0)\n",
        "    std = np.std(X_train, axis=0)\n",
        "    X_train = (X_train - mean)/std\n",
        "    \n",
        "    mean = np.mean(X_validation, axis=0)\n",
        "    std = np.std(X_validation, axis=0)    \n",
        "    X_validation = (X_validation - mean)/std\n",
        "    \n",
        "    mean = np.mean(X_test, axis=0)\n",
        "    std = np.std(X_test, axis=0)\n",
        "    X_test = (X_test - mean)/std\n",
        "    \n",
        "    \n",
        "    n_classes = 25\n",
        "    y_train = keras.utils.to_categorical(y_train, n_classes)\n",
        "    print(y_train)\n",
        "    y_validation = keras.utils.to_categorical(y_validation, n_classes)\n",
        "    y_test = keras.utils.to_categorical(y_test, n_classes)\n",
        "    \n",
        "    # create network\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
        "    print(input_shape)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=\"adam\",\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, \n",
        "                        y_train, \n",
        "                        validation_data=(X_validation, y_validation), \n",
        "                        batch_size=64, \n",
        "                        epochs=10)\n",
        "\n",
        "    # plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "    # pick a sample to predict from the test set\n",
        "    X_to_predict = X_test[100]\n",
        "    y_to_predict = y_test[100]\n",
        "\n",
        "    # predict sample\n",
        "    predict(model, X_to_predict, y_to_predict)"
      ],
      "metadata": {
        "id": "LN2xLIgmpSTB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}